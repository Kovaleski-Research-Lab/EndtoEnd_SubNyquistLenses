{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5518d3f-f1ca-46b7-93b6-0f005693c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "#import seaborn as sn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import seed_everything\n",
    "from torchmetrics.functional import mean_squared_error as mse\n",
    "from torchmetrics.functional import peak_signal_noise_ratio as psnr\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "sys.path.append('../')\n",
    "from utils import parameter_manager, model_loader\n",
    "from core import datamodule, lrn, modulator, propagator, classifiers, cooperative\n",
    "\n",
    "#plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310e500-00b5-482b-b9a9-b0dff5ce63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "params = yaml.load(open('../config.yaml'), Loader = yaml.FullLoader)\n",
    "params['batch_size'] = 1\n",
    "params['distance'] = torch.tensor(0.01)\n",
    "params['path_root'] = '/cgi/data/erdc_xai/resolution_constrained_deep_optics/'\n",
    "os.environ['TORCH_HOME'] = '/cgi/data/erdc_xai/resolution_constrained_deep_optics/pretrained_models/'\n",
    "pm = parameter_manager.Parameter_Manager(params = params)\n",
    "# Load in the test dataset\n",
    "pm.data_split = \"mnist_1000perClass\"\n",
    "datamod = datamodule.select_data(pm.params_datamodule)\n",
    "datamod.setup()\n",
    "dataloader_train_1000perClass = datamod.train_dataloader()\n",
    "dataloader_test = datamod.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f879f2d0-e840-4e9c-803e-119bc0ee774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 500\n",
    "for i,(image,target) in enumerate(dataloader_test):\n",
    "    if i == number:\n",
    "        break\n",
    "batch = (image,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fa116-4257-40c5-8910-1b88863dcb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.squeeze().abs().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f958eee-0c69-4ef0-82e6-409efb2b5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cooperative = '/cgi/data/erdc_xai/resolution_constrained_deep_optics/my_models/cooperative/'\n",
    "path_lrn = '/cgi/data/erdc_xai/resolution_constrained_deep_optics/my_models/lrn/'\n",
    "\n",
    "lrn_lensInit_folders = os.listdir(os.path.join(path_lrn, \"lensInit\"))\n",
    "lrn_lensInit_folders.sort()\n",
    "\n",
    "lrn_randomInit_folders = os.listdir(os.path.join(path_lrn, \"randomInit\"))\n",
    "lrn_randomInit_folders.sort()\n",
    "\n",
    "cooperative_analyticalLrn_folders = os.listdir(os.path.join(path_cooperative, \"analyticalLensInit\"))\n",
    "cooperative_analyticalLrn_folders.sort()\n",
    "\n",
    "cooperative_analyticalLrnOptim_folders = os.listdir(os.path.join(path_cooperative, \"analyticalLensInit_optimizable\"))\n",
    "cooperative_analyticalLrnOptim_folders.sort()\n",
    "\n",
    "cooperative_randomLrnOptim_folders = os.listdir(os.path.join(path_cooperative, \"randomLensInit_optimizable\"))\n",
    "cooperative_randomLrnOptim_folders.sort()\n",
    "\n",
    "cooperative_trainedLrnLens_folders = os.listdir(os.path.join(path_cooperative, \"trainedLrn_lensInit\"))\n",
    "cooperative_trainedLrnLens_folders.sort()\n",
    "\n",
    "cooperative_trainedLrnRandom_folders = os.listdir(os.path.join(path_cooperative, \"trainedLrn_randomInit\"))\n",
    "cooperative_trainedLrnRandom_folders.sort()\n",
    "\n",
    "print(len(lrn_lensInit_folders))\n",
    "print(len(lrn_randomInit_folders))\n",
    "print(len(cooperative_analyticalLrn_folders))\n",
    "print(len(cooperative_analyticalLrnOptim_folders))\n",
    "print(len(cooperative_randomLrnOptim_folders))\n",
    "print(len(cooperative_trainedLrnLens_folders))\n",
    "print(len(cooperative_trainedLrnRandom_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f21e15-8d2a-40b9-92a3-aac03ccb7265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances = []\n",
    "for f in lrn_lensInit_folders:\n",
    "    distances.append(torch.tensor(float(f.split('_')[-1])))   \n",
    "distances = torch.stack(distances)\n",
    "print(distances)\n",
    "integer = 20\n",
    "distances = [torch.round(distances[i*integer],decimals=3) for i in range(0,len(distances)//integer)]\n",
    "distances.append(torch.tensor(float(0.1)))\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6095a-80cb-4846-b52e-5a8d8d0458e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_distances = np.array([0.0199, 0.0352, 0.0361, 0.0406, 0.0433, 0.0442, 0.046 , 0.0505,\n",
    "       0.0514, 0.0532, 0.055 , 0.0559, 0.0577, 0.0586, 0.0595, 0.0613,\n",
    "       0.0649, 0.0658, 0.0676, 0.0685, 0.0694, 0.0712, 0.073 , 0.0739,\n",
    "       0.0757, 0.0766, 0.082 , 0.0829, 0.0847, 0.0865, 0.0901, 0.0937,\n",
    "       0.0946, 0.0955, 0.0991, 0.1   ])\n",
    "\n",
    "good_distances = np.array([0.01  , 0.0109, 0.0118, 0.0127, 0.0136, 0.0145, 0.0154, 0.0163,\n",
    "       0.0172, 0.0181, 0.019 , 0.0208, 0.0217, 0.0226, 0.0235, 0.0244,\n",
    "       0.0253, 0.0262, 0.0271, 0.028 , 0.0289, 0.0298, 0.0307, 0.0316,\n",
    "       0.0325, 0.0334, 0.0343, 0.037 , 0.0379, 0.0388, 0.0397, 0.0415,\n",
    "       0.0424, 0.0451, 0.0469, 0.0478, 0.0487, 0.0496, 0.0523, 0.0541,\n",
    "       0.0568, 0.0604, 0.0622, 0.0631, 0.064 , 0.0667, 0.0703, 0.0721,\n",
    "       0.0748, 0.0775, 0.0784, 0.0793, 0.0802, 0.0811, 0.0838, 0.0856,\n",
    "       0.0874, 0.0883, 0.0892, 0.091 , 0.0919, 0.0928, 0.0964, 0.0973,\n",
    "       0.0982])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a5a0b-3bfd-47c9-bc8b-8c03d56f1ea5",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b3254-eed9-4d65-8ef4-52a3a6147203",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_distances = np.asarray([\n",
    "'0.0199',\n",
    "'0.0316',\n",
    "'0.0433',\n",
    "'0.0550',\n",
    "'0.0667',\n",
    "'0.0784',\n",
    "'0.0901',\n",
    "'0.1000'], dtype='U6')\n",
    "\n",
    "float_distances  = np.asarray([\n",
    "0.0199,\n",
    "0.0316,\n",
    "0.0433,\n",
    "0.0550,\n",
    "0.0667,\n",
    "0.0784,\n",
    "0.0901,\n",
    "0.1000])\n",
    "\n",
    "\n",
    "print((float_distances) * 50)\n",
    "#output_wavefronts, amplitudes, normalized_amplitudes, images, normalized_images, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e349268-c426-49f1-b40c-2ebff80c843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "\n",
    "for f in tqdm(cooperative_analyticalLrn_folders):\n",
    "    #print(f)\n",
    "    if (np.repeat(f.split('_')[-1],len(image_distances)) == image_distances).any():\n",
    "        checkpoint = os.path.join(path_cooperative,\"analyticalLensInit\",f,'epoch=4-step=6250.ckpt')\n",
    "        \n",
    "        model = cooperative.CooperativeOptimization.load_from_checkpoint(checkpoint)        \n",
    "                \n",
    "        outputs['{}'.format(f)] = model.shared_step(batch, 0), model.lrn.layers[1].phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee45db-dc5d-4a1e-ba30-3e43678a3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,4, figsize=(10,10))\n",
    "\n",
    "images = []\n",
    "lenses = []\n",
    "\n",
    "for i,k in enumerate(outputs):\n",
    "    col = i%2\n",
    "    row = i//2\n",
    "    \n",
    "    #print(row,col)\n",
    "    \n",
    "    image = outputs[k][0][0][4].squeeze()\n",
    "    lens = outputs[k][-1].detach().squeeze() % (np.pi *2)\n",
    "    \n",
    "    images.append(image)\n",
    "    lenses.append(lens)\n",
    "\n",
    "    \n",
    "ax[0][0].imshow(lenses[0])\n",
    "ax[0][1].imshow(images[0])\n",
    "\n",
    "ax[0][2].imshow(lenses[1])\n",
    "ax[0][3].imshow(images[1])\n",
    "\n",
    "ax[1][0].imshow(lenses[2])\n",
    "ax[1][1].imshow(images[2])\n",
    "\n",
    "ax[1][2].imshow(lenses[3])\n",
    "ax[1][3].imshow(images[3])\n",
    "\n",
    "ax[2][0].imshow(lenses[4])\n",
    "ax[2][1].imshow(images[4])\n",
    "\n",
    "ax[2][2].imshow(lenses[5])\n",
    "ax[2][3].imshow(images[5])\n",
    "\n",
    "ax[3][0].imshow(lenses[6])\n",
    "ax[3][1].imshow(images[6])\n",
    "\n",
    "ax[3][2].imshow(lenses[7])\n",
    "ax[3][3].imshow(images[7])\n",
    "    \n",
    "for i,row in enumerate(ax):\n",
    "    for j,col in enumerate(row):\n",
    "        ax[i,j].axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "fig.savefig(\"baseline_images_lenses.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4811e-56f6-4c15-8809-d1ccf811000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "\n",
    "for f in tqdm(cooperative_analyticalLrnOptim_folders):\n",
    "    #print(f)\n",
    "    if (np.repeat(f.split('_')[-1],len(image_distances)) == image_distances).any():\n",
    "        f = f.split('_')[-1]\n",
    "        \n",
    "        checkpoint = os.path.join(path_cooperative,\"analyticalLensInit_optimizable\",'coop_optim_{}'.format(f),'epoch=4-step=6250.ckpt')\n",
    "        \n",
    "        model = cooperative.CooperativeOptimization.load_from_checkpoint(checkpoint)\n",
    "        \n",
    "        #model = cooperative.CooperativeOptimization(pm.params_model_cooperative, pm.params_model_lrn, pm.params_propagator,\n",
    "        #                                   pm.params_modulator, pm.params_model_classifier, pm.all_paths)\n",
    "        #checkpoint = torch.load(checkpoint)\n",
    "        #checkpoint = torch.load(checkpoint)\n",
    "        #state_dict = checkpoint['state_dict']\n",
    "\n",
    "        #model = cooperative.CooperativeOptimization(pm.params_model_cooperative, pm.params_model_lrn, pm.params_propagator,\n",
    "        #                                   pm.params_modulator, pm.params_model_classifier, pm.all_paths)        \n",
    "        \n",
    "        #model.load_state_dict(state_dict)\n",
    "        \n",
    "        # model = cooperative.CooperativeOptimization.load_from_checkpoint(checkpoint, \n",
    "        #                                      params_model_cooperative = pm.params_model_cooperative,\n",
    "        #                                      params_model_lrn = pm.params_model_lrn, \n",
    "        #                                      params_propagator = pm.params_propagator, \n",
    "        #                                      params_modulator = pm.params_modulator, \n",
    "        #                                      params_model_classifier = pm.params_model_classifier, \n",
    "        #                                      all_paths = pm.all_paths)\n",
    "\n",
    "        outputs['{}'.format(f)] = model.shared_step(batch, 0), model.lrn.layers[1].phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06579a-4abe-4f53-ab33-0676ebc9c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,4, figsize=(10,10))\n",
    "\n",
    "images = []\n",
    "lenses = []\n",
    "\n",
    "for i,k in enumerate(outputs):\n",
    "    col = i%2\n",
    "    row = i//2\n",
    "    \n",
    "    #print(row,col)\n",
    "    \n",
    "    image = outputs[k][0][0][3].squeeze().detach()\n",
    "    lens = outputs[k][-1].detach().squeeze() % (np.pi *2)\n",
    "    \n",
    "    images.append(image)\n",
    "    lenses.append(lens)\n",
    "\n",
    "    \n",
    "ax[0][0].imshow(lenses[0])\n",
    "ax[0][1].imshow(images[0])\n",
    "\n",
    "ax[0][2].imshow(lenses[1])\n",
    "ax[0][3].imshow(images[1])\n",
    "\n",
    "ax[1][0].imshow(lenses[2])\n",
    "ax[1][1].imshow(images[2])\n",
    "\n",
    "ax[1][2].imshow(lenses[3])\n",
    "ax[1][3].imshow(images[3])\n",
    "\n",
    "ax[2][0].imshow(lenses[4])\n",
    "ax[2][1].imshow(images[4])\n",
    "\n",
    "ax[2][2].imshow(lenses[5])\n",
    "ax[2][3].imshow(images[5])\n",
    "\n",
    "ax[3][0].imshow(lenses[6])\n",
    "ax[3][1].imshow(images[6])\n",
    "\n",
    "ax[3][2].imshow(lenses[7])\n",
    "ax[3][3].imshow(images[7])\n",
    "    \n",
    "for i,row in enumerate(ax):\n",
    "    for j,col in enumerate(row):\n",
    "        ax[i,j].axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "fig.savefig(\"analyticalInit_classifierOptimized.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592196c-4217-4d10-97b4-9a7010f372ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "\n",
    "for f in tqdm(cooperative_randomLrnOptim_folders):\n",
    "    #print(f)\n",
    "    if (np.repeat(f.split('_')[-1],len(image_distances)) == image_distances).any():\n",
    "        f = f.split('_')[-1]\n",
    "        \n",
    "        checkpoint = os.path.join(path_cooperative,\"randomLensInit_optimizable\",'coop_optim_{}'.format(f),'epoch=4-step=6250.ckpt')\n",
    "        \n",
    "        model = cooperative.CooperativeOptimization.load_from_checkpoint(checkpoint)\n",
    "        \n",
    "        #model = cooperative.CooperativeOptimization(pm.params_model_cooperative, pm.params_model_lrn, pm.params_propagator,\n",
    "        #                                   pm.params_modulator, pm.params_model_classifier, pm.all_paths)\n",
    "        #checkpoint = torch.load(checkpoint)\n",
    "        #checkpoint = torch.load(checkpoint)\n",
    "        #state_dict = checkpoint['state_dict']\n",
    "\n",
    "        #model = cooperative.CooperativeOptimization(pm.params_model_cooperative, pm.params_model_lrn, pm.params_propagator,\n",
    "        #                                   pm.params_modulator, pm.params_model_classifier, pm.all_paths)        \n",
    "        \n",
    "        #model.load_state_dict(state_dict)\n",
    "        \n",
    "        # model = cooperative.CooperativeOptimization.load_from_checkpoint(checkpoint, \n",
    "        #                                      params_model_cooperative = pm.params_model_cooperative,\n",
    "        #                                      params_model_lrn = pm.params_model_lrn, \n",
    "        #                                      params_propagator = pm.params_propagator, \n",
    "        #                                      params_modulator = pm.params_modulator, \n",
    "        #                                      params_model_classifier = pm.params_model_classifier, \n",
    "        #                                      all_paths = pm.all_paths)\n",
    "\n",
    "        outputs['{}'.format(f)] = model.shared_step(batch, 0), model.lrn.layers[1].phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2f9a2-e87a-4e5b-87c1-1c27fe89e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,4, figsize=(10,10))\n",
    "\n",
    "images = []\n",
    "lenses = []\n",
    "\n",
    "for i,k in enumerate(outputs):\n",
    "    col = i%2\n",
    "    row = i//2\n",
    "    \n",
    "    #print(row,col)\n",
    "    \n",
    "    image = outputs[k][0][0][3].squeeze().detach()\n",
    "    lens = outputs[k][-1].detach().squeeze() % (np.pi *2)\n",
    "    \n",
    "    images.append(image)\n",
    "    lenses.append(lens)\n",
    "\n",
    "    \n",
    "ax[0][0].imshow(lenses[0])\n",
    "ax[0][1].imshow(images[0])\n",
    "\n",
    "ax[0][2].imshow(lenses[1])\n",
    "ax[0][3].imshow(images[1])\n",
    "\n",
    "ax[1][0].imshow(lenses[2])\n",
    "ax[1][1].imshow(images[2])\n",
    "\n",
    "ax[1][2].imshow(lenses[3])\n",
    "ax[1][3].imshow(images[3])\n",
    "\n",
    "ax[2][0].imshow(lenses[4])\n",
    "ax[2][1].imshow(images[4])\n",
    "\n",
    "ax[2][2].imshow(lenses[5])\n",
    "ax[2][3].imshow(images[5])\n",
    "\n",
    "ax[3][0].imshow(lenses[6])\n",
    "ax[3][1].imshow(images[6])\n",
    "\n",
    "ax[3][2].imshow(lenses[7])\n",
    "ax[3][3].imshow(images[7])\n",
    "    \n",
    "for i,row in enumerate(ax):\n",
    "    for j,col in enumerate(row):\n",
    "        ax[i,j].axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "fig.savefig(\"randomInit_classifierOptimized.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61abe364-afd9-477a-85f8-bd686ccb7e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "\n",
    "for f in tqdm(lrn_lensInit_folders):\n",
    "    #print(f)\n",
    "    if (np.repeat(f.split('_')[-1],len(image_distances)) == image_distances).any():\n",
    "        f = f.split('_')[-1]\n",
    "        \n",
    "        checkpoint = os.path.join(path_lrn,\"lensInit\",'lrn_{}'.format(f),'epoch=4-step=6250.ckpt')\n",
    "        \n",
    "        model = lrn.LRN.load_from_checkpoint(checkpoint)\n",
    "        \n",
    "        #model = cooperative.CooperativeOptimization(pm.params_model_cooperative, pm.params_model_lrn, pm.params_propagator,\n",
    "        #                                   pm.params_modulator, pm.params_model_classifier, pm.all_paths)\n",
    "        #checkpoint = torch.load(checkpoint)\n",
    "        #checkpoint = torch.load(checkpoint)\n",
    "        #state_dict = checkpoint['state_dict']\n",
    "\n",
    "        #model = cooperative.CooperativeOptimization(pm.params_model_cooperative, pm.params_model_lrn, pm.params_propagator,\n",
    "        #                                   pm.params_modulator, pm.params_model_classifier, pm.all_paths)        \n",
    "        \n",
    "        #model.load_state_dict(state_dict)\n",
    "        \n",
    "        # model = cooperative.CooperativeOptimization.load_from_checkpoint(checkpoint, \n",
    "        #                                      params_model_cooperative = pm.params_model_cooperative,\n",
    "        #                                      params_model_lrn = pm.params_model_lrn, \n",
    "        #                                      params_propagator = pm.params_propagator, \n",
    "        #                                      params_modulator = pm.params_modulator, \n",
    "        #                                      params_model_classifier = pm.params_model_classifier, \n",
    "        #                                      all_paths = pm.all_paths)\n",
    "\n",
    "        outputs['{}'.format(f)] = model.shared_step(batch, 0), model.layers[1].phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ae0f3-fd72-4f39-8f00-d8dddcfe23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,4, figsize=(10,10))\n",
    "\n",
    "images = []\n",
    "lenses = []\n",
    "\n",
    "for i,k in enumerate(outputs):\n",
    "    col = i%2\n",
    "    row = i//2\n",
    "    \n",
    "    #print(row,col)\n",
    "    \n",
    "    image = outputs[k][0][3].squeeze().detach()\n",
    "    lens = outputs[k][-1].detach().squeeze() % (np.pi *2)\n",
    "    \n",
    "    images.append(image)\n",
    "    lenses.append(lens)\n",
    "\n",
    "    \n",
    "ax[0][0].imshow(lenses[0])\n",
    "ax[0][1].imshow(images[0])\n",
    "\n",
    "ax[0][2].imshow(lenses[1])\n",
    "ax[0][3].imshow(images[1])\n",
    "\n",
    "ax[1][0].imshow(lenses[2])\n",
    "ax[1][1].imshow(images[2])\n",
    "\n",
    "ax[1][2].imshow(lenses[3])\n",
    "ax[1][3].imshow(images[3])\n",
    "\n",
    "ax[2][0].imshow(lenses[4])\n",
    "ax[2][1].imshow(images[4])\n",
    "\n",
    "ax[2][2].imshow(lenses[5])\n",
    "ax[2][3].imshow(images[5])\n",
    "\n",
    "ax[3][0].imshow(lenses[6])\n",
    "ax[3][1].imshow(images[6])\n",
    "\n",
    "ax[3][2].imshow(lenses[7])\n",
    "ax[3][3].imshow(images[7])\n",
    "    \n",
    "for i,row in enumerate(ax):\n",
    "    for j,col in enumerate(row):\n",
    "        ax[i,j].axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "fig.savefig(\"lrn_lensInit.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73196e-3181-40e8-be9b-1b57a1ffcb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "\n",
    "for f in tqdm(lrn_randomInit_folders):\n",
    "    #print(f)\n",
    "    if (np.repeat(f.split('_')[-1],len(image_distances)) == image_distances).any():\n",
    "        f = f.split('_')[-1]\n",
    "        \n",
    "        checkpoint = os.path.join(path_lrn,\"randomInit\",'lrn_randomInit_{}'.format(f),'epoch=4-step=6250.ckpt')\n",
    "        \n",
    "        model = lrn.LRN.load_from_checkpoint(checkpoint)\n",
    "        \n",
    "        #model = cooperative.CooperativeOptimization(pm.params_model_cooperative, pm.params_model_lrn, pm.params_propagator,\n",
    "        #                                   pm.params_modulator, pm.params_model_classifier, pm.all_paths)\n",
    "        #checkpoint = torch.load(checkpoint)\n",
    "        #checkpoint = torch.load(checkpoint)\n",
    "        #state_dict = checkpoint['state_dict']\n",
    "\n",
    "        #model = cooperative.CooperativeOptimization(pm.params_model_cooperative, pm.params_model_lrn, pm.params_propagator,\n",
    "        #                                   pm.params_modulator, pm.params_model_classifier, pm.all_paths)        \n",
    "        \n",
    "        #model.load_state_dict(state_dict)\n",
    "        \n",
    "        # model = cooperative.CooperativeOptimization.load_from_checkpoint(checkpoint, \n",
    "        #                                      params_model_cooperative = pm.params_model_cooperative,\n",
    "        #                                      params_model_lrn = pm.params_model_lrn, \n",
    "        #                                      params_propagator = pm.params_propagator, \n",
    "        #                                      params_modulator = pm.params_modulator, \n",
    "        #                                      params_model_classifier = pm.params_model_classifier, \n",
    "        #                                      all_paths = pm.all_paths)\n",
    "\n",
    "        outputs['{}'.format(f)] = model.shared_step(batch, 0), model.layers[1].phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55551d4-f046-4aa4-a2f3-da21404d7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,4, figsize=(10,10))\n",
    "\n",
    "images = []\n",
    "lenses = []\n",
    "\n",
    "for i,k in enumerate(outputs):\n",
    "    col = i%2\n",
    "    row = i//2\n",
    "    \n",
    "    #print(row,col)\n",
    "    \n",
    "    image = outputs[k][0][3].squeeze().detach()\n",
    "    lens = outputs[k][-1].detach().squeeze() % (np.pi *2)\n",
    "    \n",
    "    images.append(image)\n",
    "    lenses.append(lens)\n",
    "\n",
    "    \n",
    "ax[0][0].imshow(lenses[0])\n",
    "ax[0][1].imshow(images[0])\n",
    "\n",
    "ax[0][2].imshow(lenses[1])\n",
    "ax[0][3].imshow(images[1])\n",
    "\n",
    "ax[1][0].imshow(lenses[2])\n",
    "ax[1][1].imshow(images[2])\n",
    "\n",
    "ax[1][2].imshow(lenses[3])\n",
    "ax[1][3].imshow(images[3])\n",
    "\n",
    "ax[2][0].imshow(lenses[4])\n",
    "ax[2][1].imshow(images[4])\n",
    "\n",
    "ax[2][2].imshow(lenses[5])\n",
    "ax[2][3].imshow(images[5])\n",
    "\n",
    "ax[3][0].imshow(lenses[6])\n",
    "ax[3][1].imshow(images[6])\n",
    "\n",
    "ax[3][2].imshow(lenses[7])\n",
    "ax[3][3].imshow(images[7])\n",
    "    \n",
    "for i,row in enumerate(ax):\n",
    "    for j,col in enumerate(row):\n",
    "        ax[i,j].axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "fig.savefig(\"lrn_randomInit.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a858b03-ba86-434c-bbb7-0141924f68a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
